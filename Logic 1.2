week3
1.Identifying distribution of data
The very first task is to find the distribution of the given data. For that we have imported the data to RStudio and by using fitdistrplus package we tried to fit our data to any possible distribution. The code goes as follows:
> myfile<-read.csv("file.csv")   #myfile contains 4114 observations of 1 variable (there are 886 missing values in week 3 data)
> install.packages("ggplot2")    #installing required packages for visualization and distribution fitting 
> install.packages("fitdistrplus")
> plotdist(myfile$data, histo = TRUE, demp = TRUE)   #empirical density curve and cdf curve plot has been plotted showing negatively skewed distribution
> descdist(myfile$data,boot=1000)    #Cullen and Frey graph has been plotted showing the type of distribution. Skewness and kurtosis value gets plotted on this graph
Output:
summary statistics
------
min:  1   max:  7 
median:  5 
mean:  4.687409   #mean is less than median showing negatively skewed distribution
estimated sd:  1.711221 
estimated skewness:  -0.3765928   #negative sign and magnitude being close to zero indicates that data is showing low level negative skewness 
estimated kurtosis:  2.211002 
From cullen and frey graph, it can be interpreted since bootstraped values(sanple values taken with replacement) lies very close to observed values suggesting that data does not conform to any possible distribution rather it is taken randomly. But since data shows low negative skewness of magnitude 0.376, we have tried to visualise how close our data fits the symmetrical distribution namely normal and uniform distribution. For this the code goes as follows:
> fu<-fitdist(myfile$data,"unif")  #fitting data to uniform distribution
> fn<-fitdist(myfile$data,"norm")  #fitting data to normal distribution
> par(mfrow=c(2,2))
> plot.legend<-c("unif","norm")
> denscomp(list(fu,fn),legendtext = plot.legend)
Histogram and theoretical densities graph thus plotted do not give satisfactory results as both these distributions do not fit the data well. Hence our data has been taken randomly.
The code for obtaining the measures of central tendency, dispersion and moments goes as follows:
> quantile(myfile$data)   #for quartiles
  0%  25%  50%  75% 100% 
   1    3    5    6    7 
> max(myfile$data)-min(myfile$data)   #for range
[1] 6
> sd(myfile$data)   #for standard deviation
[1] 1.711221
> var(myfile$data)  #for variance
[1] 2.928278
moment(myfile$data,order=2,center=TRUE)  #central moment of order 2 same as variance
[1] 2.927566
> moment(myfile$data,order=3,center=TRUE) #central moment of order 3 being negative
[1] -1.885705
> moment(myfile$data,order=4,center=TRUE) #central moment of order 4
[1] 18.94543
2. Imputation
For imputing the missing values present in week 3 data, MICE package has been installed in RStudio which will use the technique of predictive mean matching (pmm) to impute. The code for this goes as follows:
> mybook<-read.csv("Book.csv")  #importing the week 3 dataset to RStudio
> head(mybook)  
   M  T W TH  F
1  3  4 5  6  7
2  3  4 5  6  7
3  5  6 6  7 NA
4  1 NA 3  4  5
5 NA  6 6  7 NA
6  1 NA 3 NA  5
> install.packages("mice")
> md.pattern(mybook)   #md.pattern function of MICE package depicts clearly the missing and non missing values of the data
      W  TH   F   T   M    
364   1   1   1   1   1   0     #there are 364 observations with no missing value
86    1   1   1   1   0   1     #there are 86 observations with no value in Monday column
86    1   1   1   0   1   1
31    1   1   1   0   0   2
74    1   1   0   1   1   1
26    1   1   0   1   0   2
21    1   1   0   0   1   2
79    1   0   1   1   1   1
20    1   0   1   1   0   2
21    1   0   1   0   1   2
1     1   0   1   0   0   3
25    1   0   0   1   1   2
66    0   1   1   1   1   1
22    0   1   1   1   0   2
22    0   1   1   0   1   2
27    0   1   0   1   1   2
1     0   1   0   0   1   3
26    0   0   1   1   1   2
1     0   0   0   1   1   3
1     0   0   0   1   0   4
    166 174 176 183 187 886
> imputeddata <- mice(mybook, m=5, maxit = 50, method = 'pmm', seed = 500)   #m gives the number of datasets generated that is 5
> summary(imputeddata)
Class: mids
Number of multiple imputations:  5 
Imputation methods:
    M     T     W    TH     F 
"pmm" "pmm" "pmm" "pmm" "pmm" 
PredictorMatrix:
   M T W TH F
M  0 1 1  1 1
T  1 0 1  1 1
W  1 1 0  1 1
TH 1 1 1  0 1
F  1 1 1  1 0
> imputeddata$imp$M  #showing imputed values generated in 5 different datasets of column M. Similarly imputed values are generated for other days too.
    1 2 3 4 5
5   5 5 5 5 5
14  2 2 2 2 2
18  2 2 2 2 2
22  3 3 3 3 3
25  4 4 4 4 4
28  1 1 2 1 1
For deciding which dataset to be used as imputed dataset, we have made a density plot showing the similarity between observed values and 5 new datasets values of each column. From density plot it can be inferred that nearly all the datasets lie close to observed dataset so we can use any of the 5 datasets generated as resulting imputed datset.
> densityplot(imputeddata)
> completedata<-complete(imputeddata,3)  #complete function has compiled all the imputed values using 3rd dataset.
> head(completedata)
  M T W TH F
1 3 4 5  6 7
2 3 4 5  6 7
3 5 6 6  7 7
4 1 2 3  4 5
5 5 6 6  7 7
6 1 2 3  4 5
