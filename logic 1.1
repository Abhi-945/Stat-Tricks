
## week 2 
Date 30/dec/2020

#reading the data
df = pd.read_excel("/content/drive/MyDrive/do well/week 2/week_2_data.xlsx")


df.head()
# to check the frist 5 rows of the dataset

df.shape
# to check the shape of the dataset

#to describe the data
df.describe()

#checking the names of the columns
df.columns

we found that the columns names is not in correct form so we rename the column names

#to rename the names of columns

data = df.rename(columns = {'Weeks': "Weeks", 'W1' : "W1M", 'Unnamed: 2' : "W1T", 'Unnamed: 3': "W1W", 'Unnamed: 4': "W1Th", 'Unnamed: 5':"W1F",
       'W2': "W2M", 'Unnamed: 7':'W2T', 'Unnamed: 8':"W2W", 'Unnamed: 9':"W2Th", 'Unnamed: 10': "W2F"})
       
## drop the unusable 
data =  data.drop("Weeks", axis = 1 ) # dropping the column ("Weeks")
data = data.drop([0]) # droping the first row (days)

## inspecting the modified data
dat.head()

data.to_csv("/content/drive/MyDrive/do well/week 2/week_2_data.csv") # saving it to csv form

data.describe() # #Display some descriptive statistics for the  columns

# how many missing values present in the data?
data.isnull().sum()

#information of the data set
data.info()#checking for null values too.

we have observed that the data type is of object that should we int form so now we will convert the datatype in int

# so we have converted the data into float 
for i in data.columns:
  data[i] = data[i].astype(float)
  
  
after converting the data type we deals with missing values 

we use mean method to deal with missing values
just find the mean of each row and fill that value at place of missing values
we use mean method to deal with missing values because the number of missing observations is low.
as we are dealing with each and every row:

mean = round(data.mean(axis=1))# find the mean of each row
for i,col in enumerate(data):# filling up missing values using the mean of that particular row
    data.iloc[:, i] = data.iloc[:,i].fillna(mean)


#day2(thusday)
date 31/dec/2020

I have observed that most of the data is filled sequentially

    for a particular week and row

    it is filled like that
        2,3,4,5,6
        3,3,4,4,5,5
        2,2,3,4,4

    and is filled in asscending order for the particular week.

    so we will deal with imputing missing values according to that.

   #After observing the data we find that the missing number of data points to include closer to the missing value
    so we use KNN imputer
# import the KNNimputer class 
from sklearn.impute import KNNImputer 
# create an object for KNNImputer 
imputer = KNNImputer(n_neighbors=2) 
After_imputation = imputer.fit_transform(data_week_2) 


we have seperated the week twoo dat to fill the null values and then join the data of both the weeks week1 and week2 

data_imputed = data_week_1.join(data_week_2_imputed)
data_imputed.head()
 and this imputes with the best reesult of our data 
 
 we are procceding for furthur tasks
 
 
 Day3
 date 1/jan/2021
 
 analysis the type of distribustion and prepeare the data again after finding missing values.
data_week_2 = data[data.columns[5:10]]
data_week_1 = data[data.columns[0:5]]
seperated the data of both the weeks 


then mmerge them up down:

after that i try a lot of method 
I them some are overfitting and some are under fitting
I tried these methods:
# Spot-Check Algorithms
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))

# Standardize the dataset
pipelines = []
pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LogisticRegression())])))
pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))
pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',KNeighborsClassifier())])))
pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',DecisionTreeClassifier())])))
pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',GaussianNB())])))
pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))


some of them are under fitting that means their accuray percent is very low
and some of them are over fitting  that mean thier accuracy is 1
I also tried to transpose the data frame and then aply different method but they got sample size issue and some of thm does not even worked

after doing a lot of research with data and models 


I found one model model that gives fits best with our dataset that is KNeighborsRegressor

so i go on with that and  made the prediction for next week but of 600 students and kept the same in a csv file.
code that i used is 


import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split


df = pd.read_csv("/content/drive/MyDrive/do well/week 2/final_data.csv")
print(df.shape)
df.head()


from sklearn.model_selection import train_test_split
train , test = train_test_split(df, test_size = 0.3)

x_train = train.drop('Unnamed: 0', axis=1)
y_train = train['W']

x_test = test.drop('Unnamed: 0', axis = 1)
y_test = test['W']


from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)

x_test_scaled = scaler.fit_transform(x_test)
x_test = pd.DataFrame(x_test_scaled)


#import required packages
from sklearn import neighbors
from sklearn.metrics import mean_squared_error 
from math import sqrt
import matplotlib.pyplot as plt
%matplotlib inline


rmse_val = [] #to store rmse values for different k
for K in range(20):
    K = K+1
    model = neighbors.KNeighborsRegressor(n_neighbors = K)

    model.fit(x_train, y_train)  #fit the model
    pred=model.predict(x_test) #make prediction on test set
    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse
    rmse_val.append(error) #store rmse values
    print('RMSE value for k= ' , K , 'is:', error)
    
    
    
 #plotting the rmse values against k values
curve = pd.DataFrame(rmse_val)  
curve.plot()


# prepearing the sample size

dd = pd.read_csv("/content/drive/MyDrive/do well/week 2/final_data.csv")
dd = dd.drop(dd.index[600:2000])
dd = dd.drop("Unnamed: 0", axis = 1)
 save the sample size
 
 dd.to_csv("/content/drive/MyDrive/do well/week 2/sample_data.csv")
 
 #reading test and submission files
test = pd.DataFrame(x_test_scaled)
submission = dd
submission['M'] = test[0]
submission['T'] = test[1]

test = pd.get_dummies(test)
test_scaled = scaler.fit_transform(test)
test = pd.DataFrame(test_scaled)


#predicting on the test set and saving file
predict = model.predict(test)
submission['M'] = predict
submission["T"] = predict
submission['W'] = predict
submission["TH"] = predict
submission['F'] = predict
submission.to_csv('/content/drive/MyDrive/do well/predicted.csv',index=False)

# prediction on the train set and saving file

#predicting on the test set and creating submission file
predict = model.predict(train)
submission['M'] = predict
submission["T"] = predict
submission['W'] = predict
submission["TH"] = predict
submission['F'] = predict
submission.to_csv('/content/drive/MyDrive/do well/predicted_with_train.csv',index=False)





